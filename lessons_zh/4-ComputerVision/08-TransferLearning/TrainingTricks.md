# 深度学习训练技巧

随着神经网络变得更深，它们的训练过程变得越来越具有挑战性。一个主要问题是所谓的[梯度消失](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) 或[梯度爆炸](https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.)。[这篇文章](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11) 很好地介绍了这些问题。

为了使深度网络的训练更有效，可以使用一些技术。

## 将值保持在合理区间内

为了使数值计算更稳定，我们希望确保神经网络内的所有值都在合理的范围内，通常是 [-1..1] 或 [0..1]。这不是一个非常严格的要求，但浮点计算的本质是这样的，不同量级的值不能准确地一起操作。例如，如果我们添加 10<sup>-10</sup> 和 10<sup>10</sup>，我们可能会得到 10<sup>10</sup>，因为较小的值将被"转换"为与较大值相同的数量级，因此尾数将丢失。

大多数激活函数在 [-1..1] 附近具有非线性，因此将所有输入数据缩放到 [-1..1] 或 [0..1] 区间是有意义的。

## 初始权重初始化

理想情况下，我们希望值在通过网络层后保持在相同范围内。因此，以保留值分布的方式初始化权重很重要。

正态分布 **N(0,1)** 不是一个好主意，因为如果我们有 *n* 个输入，输出的标准差将是 *n*，值可能会跳出 [0..1] 区间。

经常使用以下初始化：

 * 均匀分布 -- `uniform`
 * **N(0,1/n)** -- `gaussian`
 * **N(0,1/&radic;n_in)** 保证对于零均值和标准差为 1 的输入，相同的均值/标准差将保持不变
 * **N(0,&radic;2/(n_in+n_out))** -- 所谓的 **Xavier 初始化**（`glorot`），它有助于在前向和反向传播期间将信号保持在范围内

## 批量归一化

即使有适当的权重初始化，权重在训练期间也可能变得任意大或小，它们会将信号带出适当范围。我们可以通过使用**归一化**技术之一将信号带回来。虽然有几种（权重归一化、层归一化），但最常用的是批量归一化。

**批量归一化**的思想是考虑小批量中的所有值，并基于这些值执行归一化（即减去均值并除以标准差）。它被实现为一个网络层，在应用权重之后但在激活函数之前执行此归一化。结果，我们可能会看到更高的最终准确率和更快的训练。

这里是[关于批量归一化的原始论文](https://arxiv.org/pdf/1502.03167.pdf)，[维基百科上的解释](https://en.wikipedia.org/wiki/Batch_normalization)，以及[一篇优秀的介绍性博客文章](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338)（以及[俄语版本](https://habrahabr.ru/post/309302/)）。

## Dropout

**Dropout** 是一种有趣的技术，在训练期间移除一定百分比的随机神经元。它也被实现为一个层，具有一个参数（要移除的神经元百分比，通常为 10%-50%），在训练期间，它在将输入向量传递到下一层之前将输入向量的随机元素归零。

虽然这听起来像是一个奇怪的想法，但您可以在 [`Dropout.ipynb`](Dropout.ipynb) 笔记本中看到 dropout 对训练 MNIST 数字分类器的影响。它加快了训练速度，并允许我们在更少的训练轮次中达到更高的准确率。

这种效果可以通过几种方式解释：

 * 它可以被认为是模型的随机冲击因子，使优化摆脱局部最小值
 * 它可以被视为*隐式模型平均*，因为我们可以说在 dropout 期间我们正在训练稍微不同的模型

> *有些人说，当醉酒的人试图学习某些东西时，他会在第二天早上更好地记住这一点，与清醒的人相比，因为具有一些故障神经元的大脑试图更好地适应以理解含义。我们从未测试过自己这是真的还是假的*

## 防止过拟合

深度学习的一个非常重要的方面是能够防止[过拟合](../../3-NeuralNetworks/05-Frameworks/Overfitting.md)。虽然使用非常强大的神经网络模型可能很诱人，但我们应该始终平衡模型参数的数量与训练样本的数量。

> 确保您理解我们之前介绍的[过拟合](../../3-NeuralNetworks/05-Frameworks/Overfitting.md) 概念！

有几种方法可以防止过拟合：

 * 早停 -- 持续监控验证集上的错误，并在验证错误开始增加时停止训练。
 * 显式权重衰减 / 正则化 -- 为权重的绝对高值向损失函数添加额外惩罚，这防止模型获得非常不稳定的结果
 * 模型平均 -- 训练几个模型然后平均结果。这有助于最小化方差。
 * Dropout（隐式模型平均）

## 优化器 / 训练算法

训练的另一个重要方面是选择良好的训练算法。虽然经典的**梯度下降**是一个合理的选择，但它有时可能太慢，或导致其他问题。

在深度学习中，我们使用**随机梯度下降**（SGD），这是应用于从小批量随机选择的梯度下降，从训练集中随机选择。权重使用以下公式调整：

w<sup>t+1</sup> = w<sup>t</sup> - &eta;&nabla;&lagran;

### 动量

在**动量 SGD** 中，我们保留来自先前步骤的梯度的一部分。这类似于当我们由于惯性移动到某个地方，并在不同方向受到冲击时，我们的轨迹不会立即改变，而是保持原始运动的一部分。这里我们引入另一个向量 v 来表示*速度*：

* v<sup>t+1</sup> = &gamma; v<sup>t</sup> - &eta;&nabla;&lagran;
* w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup>

这里参数 &gamma; 表示我们考虑惯性的程度：&gamma;=0 对应于经典 SGD；&gamma;=1 是纯运动方程。

### Adam、Adagrad 等

由于在每一层中我们通过某个矩阵 W<sub>i</sub> 乘以信号，取决于 ||W<sub>i</sub>||，梯度可能减小并接近 0，或无限上升。这是梯度爆炸/消失问题的本质。

此问题的解决方案之一是在方程中仅使用梯度的方向，并忽略绝对值，即

w<sup>t+1</sup> = w<sup>t</sup> - &eta;(&nabla;&lagran;/||&nabla;&lagran;||)，其中 ||&nabla;&lagran;|| = &radic;&sum;(&nabla;&lagran;)<sup>2</sup>

此算法称为 **Adagrad**。使用相同思想的其他算法：**RMSProp**、**Adam**

> **Adam** 被认为是对许多应用非常有效的算法，因此如果您不确定使用哪一个 - 使用 Adam。

### 梯度裁剪

梯度裁剪是上述思想的扩展。当 ||&nabla;&lagran;|| &le; &theta; 时，我们在权重优化中考虑原始梯度，当 ||&nabla;&lagran;|| > &theta; - 我们通过其范数除以梯度。这里 &theta; 是一个参数，在大多数情况下我们可以取 &theta;=1 或 &theta;=10。

### 学习率衰减

训练成功通常取决于学习率参数 &eta;。逻辑上假设 &eta; 的较大值导致更快的训练，这通常是我们希望在训练开始时想要的，然后较小的 &eta; 值允许我们微调网络。因此，在大多数情况下，我们希望在训练过程中减小 &eta;。

这可以通过在训练的每个轮次后将 &eta; 乘以某个数字（例如 0.98）来完成，或通过使用更复杂的**学习率调度**。

## 不同的网络架构

为您的问题选择正确的网络架构可能很棘手。通常，我们会采用已证明适用于我们的特定任务（或类似任务）的架构。这里是[神经网络架构的良好概述](https://www.topbots.com/a-brief-history-of-neural-network-architectures/) 用于计算机视觉。

> 选择对训练样本数量足够强大的架构很重要。选择太强大的模型可能导致[过拟合](../../3-NeuralNetworks/05-Frameworks/Overfitting.md)

另一种好方法是使用将自动调整到所需复杂性的架构。在某种程度上，**ResNet** 架构和 **Inception** 是自调整的。[更多关于计算机视觉架构](../07-ConvNets/CNN_Architectures.md)

