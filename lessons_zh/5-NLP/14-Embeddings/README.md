# 嵌入

## [课前测验](https://ff-quizzes.netlify.app/en/ai/quiz/27)

当基于 BoW 或 TF/IDF 训练分类器时，我们在长度为 `vocab_size` 的高维词袋向量上操作，并且我们明确地从低维位置表示向量转换为稀疏 one-hot 表示。然而，这种 one-hot 表示不是内存高效的。此外，每个单词都独立于其他单词处理，即 one-hot 编码的向量不表达单词之间的任何语义相似性。

**嵌入**的思想是用较低维的密集向量表示单词，这些向量以某种方式反映单词的语义含义。我们稍后将讨论如何构建有意义的词嵌入，但现在让我们只将嵌入视为降低词向量维度的一种方式。

因此，嵌入层将接受一个单词作为输入，并产生指定 `embedding_size` 的输出向量。在某种意义上，它与 `Linear` 层非常相似，但它将能够接受单词编号作为输入，而不是接受 one-hot 编码的向量，允许我们避免创建大的 one-hot 编码向量。

通过在我们的分类器网络中使用嵌入层作为第一层，我们可以从词袋切换到**嵌入袋**模型，我们首先将文本中的每个单词转换为相应的嵌入，然后计算所有这些嵌入上的某个聚合函数，如 `sum`、`average` 或 `max`。

![显示五个序列单词的嵌入分类器示例的图像。](images/embedding-classifier-example.png)

> 图片由作者制作

## ✍️ 练习：嵌入

在以下笔记本中继续学习：
* [使用 PyTorch 进行嵌入](EmbeddingsPyTorch.ipynb)
* [嵌入 TensorFlow](EmbeddingsTF.ipynb)

## 语义嵌入：Word2Vec

虽然嵌入层学会了将单词映射到向量表示，但是，这种表示不一定具有太多语义含义。学习向量表示会很好，使得相似单词或同义词对应于在某种向量距离（例如欧几里得距离）方面彼此接近的向量。

为了做到这一点，我们需要以特定方式在大量文本集合上预训练我们的嵌入模型。训练语义嵌入的一种方法称为 [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)。它基于用于产生单词的分布式表示的两种主要架构：

 - **连续词袋**（CBoW）— 在这种架构中，我们训练模型从周围上下文预测单词。给定 ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$，模型的目标是从 $(W_{-2},W_{-1},W_1,W_2)$ 预测 $W_0$。
 - **连续跳元**与 CBoW 相反。模型使用周围上下文单词窗口来预测当前单词。

CBoW 更快，而跳元更慢，但在表示不常见单词方面做得更好。

![显示 CBoW 和 Skip-Gram 算法将单词转换为向量的图像。](./images/example-algorithms-for-converting-words-to-vectors.png)

> 图片来自[这篇论文](https://arxiv.org/pdf/1301.3781.pdf)

Word2Vec 预训练嵌入（以及其他类似模型，如 GloVe）也可以代替神经网络中的嵌入层使用。然而，我们需要处理词汇表，因为用于预训练 Word2Vec/GloVe 的词汇表可能与我们的文本语料库中的词汇表不同。查看上面的笔记本以了解如何解决这个问题。

## 上下文嵌入

传统预训练嵌入表示（如 Word2Vec）的一个关键限制是词义消歧问题。虽然预训练嵌入可以捕获上下文中单词的一些含义，但单词的每个可能含义都编码到相同的嵌入中。这可能会在下游模型中引起问题，因为许多单词（如单词"play"）根据它们使用的上下文具有不同的含义。

例如，单词"play"在这两个不同的句子中具有完全不同的含义：

- 我去剧院看了一场**play**（戏剧）。
- John 想和他的朋友们**play**（玩）。

上面的预训练嵌入在同一个嵌入中表示单词"play"的这两种含义。为了克服这个限制，我们需要基于**语言模型**构建嵌入，该模型在大量文本语料库上训练，并且*知道*单词如何在不同的上下文中组合在一起。讨论上下文嵌入超出了本教程的范围，但我们将在课程后面讨论语言模型时回到它们。

## 结论

在本课中，您发现了如何在 TensorFlow 和 Pytorch 中构建和使用嵌入层，以更好地反映单词的语义含义。

## 🚀 挑战

Word2Vec 已用于一些有趣的应用，包括生成歌词和诗歌。查看[这篇文章](https://www.politetype.com/blog/word2vec-color-poems)，它详细介绍了作者如何使用 Word2Vec 生成诗歌。也观看[Dan Shiffmann 的这个视频](https://www.youtube.com/watch?v=LSS_bos_TPI&ab_channel=TheCodingTrain)，以发现对此技术的不同解释。然后尝试将这些技术应用于您自己的文本语料库，可能来自 Kaggle。

## [课后测验](https://ff-quizzes.netlify.app/en/ai/quiz/28)

## 复习与自主学习

阅读这篇关于 Word2Vec 的论文：[向量空间中词表示的有效估计](https://arxiv.org/pdf/1301.3781.pdf)

## [作业：笔记本](assignment.md)

