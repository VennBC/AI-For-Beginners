# 循环神经网络

## [课前测验](https://ff-quizzes.netlify.app/en/ai/quiz/31)

在之前的章节中，我们一直在使用丰富的文本语义表示和在嵌入之上的简单线性分类器。这种架构所做的是捕获句子中单词的聚合含义，但它不考虑单词的**顺序**，因为嵌入之上的聚合操作从原始文本中删除了此信息。由于这些模型无法建模单词顺序，它们无法解决更复杂或模糊的任务，如文本生成或问答。

为了捕获文本序列的含义，我们需要使用另一种神经网络架构，称为**循环神经网络**，或 RNN。在 RNN 中，我们一次一个符号地通过网络传递我们的句子，网络产生一些**状态**，然后我们再次将它与下一个符号一起传递给网络。

![RNN](./images/rnn.png)

> 图片由作者制作

给定输入标记序列 X<sub>0</sub>,...,X<sub>n</sub>，RNN 创建神经网络块序列，并使用反向传播端到端训练此序列。每个网络块接受一对 (X<sub>i</sub>,S<sub>i</sub>) 作为输入，并产生 S<sub>i+1</sub> 作为结果。最终状态 S<sub>n</sub> 或（输出 Y<sub>n</sub>）进入线性分类器以产生结果。所有网络块共享相同的权重，并使用一次反向传播通过端到端训练。

因为状态向量 S<sub>0</sub>,...,S<sub>n</sub> 通过网络传递，它能够学习单词之间的顺序依赖关系。例如，当单词 *not* 出现在序列中的某个地方时，它可以学习否定状态向量中的某些元素，从而导致否定。

> ✅ 由于上面图片中所有 RNN 块的权重是共享的，相同的图片可以表示为一个块（在右侧），具有循环反馈循环，将网络的输出状态传递回输入。

## RNN 单元的结构

让我们看看简单的 RNN 单元是如何组织的。它接受先前的状态 S<sub>i-1</sub> 和当前符号 X<sub>i</sub> 作为输入，并且必须产生输出状态 S<sub>i</sub>（有时，我们也对其他输出 Y<sub>i</sub> 感兴趣，如生成网络的情况）。

简单的 RNN 单元内部有两个权重矩阵：一个转换输入符号（让我们称它为 W），另一个转换输入状态（H）。在这种情况下，网络的输出计算为 &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b)，其中 &sigma; 是激活函数，b 是额外的偏置。

<img alt="RNN 单元结构" src="images/rnn-anatomy.png" width="50%"/>

> 图片由作者制作

在许多情况下，输入标记在进入 RNN 之前通过嵌入层传递以降低维度。在这种情况下，如果输入向量的维度是 *emb_size*，状态向量是 *hid_size* - W 的大小是 *emb_size*&times;*hid_size*，H 的大小是 *hid_size*&times;*hid_size*。

## 长短期记忆（LSTM）

经典 RNN 的主要问题之一是所谓的**梯度消失**问题。因为 RNN 在一次反向传播通过中端到端训练，它难以将错误传播到网络的第一层，因此网络无法学习远距离标记之间的关系。避免此问题的方法之一是通过使用所谓的**门**来引入**显式状态管理**。有两种众所周知的这种架构：**长短期记忆**（LSTM）和**门控中继单元**（GRU）。

![显示长短期记忆单元示例的图像](./images/long-short-term-memory-cell.svg)

> 图片来源待定

LSTM 网络的组织方式类似于 RNN，但有两个状态从层传递到层：实际状态 C 和隐藏向量 H。在每个单元，隐藏向量 H<sub>i</sub> 与输入 X<sub>i</sub> 连接，它们通过**门**控制状态 C 发生什么。每个门是具有 sigmoid 激活（范围 [0,1] 中的输出）的神经网络，当乘以状态向量时可以被认为是按位掩码。有以下门（在上图中从左到右）：

* **遗忘门**接受隐藏向量并确定我们需要忘记向量 C 的哪些分量，以及哪些要传递。
* **输入门**从输入和隐藏向量中获取一些信息并将其插入状态。
* **输出门**通过具有 *tanh* 激活的线性层转换状态，然后使用隐藏向量 H<sub>i</sub> 选择其某些分量以产生新状态 C<sub>i+1</sub>。

状态 C 的分量可以被认为是一些可以打开和关闭的标志。例如，当我们在序列中遇到名称 *Alice* 时，我们可能想要假设它指的是女性角色，并在状态中提升标志，表明我们在句子中有一个女性名词。当我们进一步遇到短语 *and Tom* 时，我们将提升标志，表明我们有一个复数名词。因此，通过操作状态，我们可以假设跟踪句子部分的语法属性。

> ✅ 理解 LSTM 内部结构的优秀资源是 Christopher Olah 的这篇优秀文章 [理解 LSTM 网络](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)。

## 双向和多层 RNN

我们讨论了在一个方向上操作的循环网络，从序列的开始到结束。这看起来很自然，因为它类似于我们阅读和听语音的方式。然而，由于在许多实际情况下我们对输入序列有随机访问，在两个方向上运行循环计算可能是有意义的。这样的网络称为**双向** RNN。在处理双向网络时，我们需要两个隐藏状态向量，每个方向一个。

循环网络，无论是单向还是双向，捕获序列内的某些模式，并可以将它们存储到状态向量中或传递到输出。与卷积网络一样，我们可以在第一层之上构建另一个循环层以捕获更高级别的模式，并从第一层提取的低级模式构建。这使我们想到**多层 RNN** 的概念，它由两个或多个循环网络组成，其中前一层的输出作为输入传递到下一层。

![显示多层长短期记忆 RNN 的图像](./images/multi-layer-lstm.jpg)

*图片来自 Fernando López 的[这篇精彩文章](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3)*

## ✍️ 练习：嵌入

在以下笔记本中继续学习：

* [使用 PyTorch 的 RNN](RNNPyTorch.ipynb)
* [使用 TensorFlow 的 RNN](RNNTF.ipynb)

## 结论

在本单元中，我们看到 RNN 可以用于序列分类，但实际上，它们可以处理更多任务，如文本生成、机器翻译等。我们将在下一个单元中考虑这些任务。

## 🚀 挑战

阅读一些关于 LSTM 的文献并考虑它们的应用：

- [网格长短期记忆](https://arxiv.org/pdf/1507.01526v1.pdf)
- [显示、注意和讲述：具有视觉注意的神经图像字幕生成](https://arxiv.org/pdf/1502.03044v2.pdf)

## [课后测验](https://ff-quizzes.netlify.app/en/ai/quiz/32)

## 复习与自主学习

- [理解 LSTM 网络](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) 由 Christopher Olah。

## [作业：笔记本](assignment.md)

